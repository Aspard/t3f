{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "#### [Open](https://colab.research.google.com/github/Bihaqo/t3f/blob/develop/docs/quick_start.ipynb) this page in an interactive mode via Google Colaboratory.\n",
    "\n",
    "In this quick starting guide we show the basics of working with t3f library. The main concept of the library is a TensorTrain object -- a compact (factorized) representation of a tensor (=multidimensional array). This is generalization of the matrix low-rank decomposition.\n",
    "\n",
    "\n",
    "To begin, [install T3F](https://t3f.readthedocs.io/en/latest/installation.html), import some libraries, and enable [eager execution mode](https://www.tensorflow.org/guide/eager) which simplifies workflow with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "try:\n",
    "    import t3f\n",
    "except ImportError:\n",
    "    # Install T3F if it's not already installed.\n",
    "    !git clone https://github.com/Bihaqo/t3f.git\n",
    "    !cd t3f; pip install .\n",
    "    import t3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with converting a dense (numpy) matrix into the TT-format, which in this case coinsides with the low-rank format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e09e84757b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert the matrix into the TT-format with TT-rank = 3 (the larger the TT-rank, the more exactly the tensor will be converted, but the more memory and time everything will take). For matrices, matrix rank coinsides with TT-rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma_tt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt3f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tt_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tt_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# a_tt stores the factorized representation of the matrix, namely it stores the matrix as a product of two smaller matrices which are called TT-cores. You can access the TT-cores directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_tt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/t3f/t3f/decompositions.py\u001b[0m in \u001b[0;36mto_tt_tensor\u001b[0;34m(tens, max_tt_rank, epsilon, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mare_tt_ranks_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_cores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/t3f/t3f/tensor_train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tt_cores, shape, tt_ranks, convert_to_tensors)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m# TODO: what does this namescope do?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorTrain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"core%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6008\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6009\u001b[0m         \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6010\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcache_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_scope_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6011\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_scope_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6012\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Generate a random dense matrix of size 3 x 4.\n",
    "a_dense = np.random.randn(3, 4)\n",
    "# Convert the matrix into the TT-format with TT-rank = 3 (the larger the TT-rank, the more exactly the tensor will be converted, but the more memory and time everything will take). For matrices, matrix rank coinsides with TT-rank.\n",
    "a_tt = t3f.to_tt_tensor(a_dense, max_tt_rank=3)\n",
    "# a_tt stores the factorized representation of the matrix, namely it stores the matrix as a product of two smaller matrices which are called TT-cores. You can access the TT-cores directly.\n",
    "print(a_tt.tt_cores)\n",
    "# To check that the convertions into the TT-format didn't change the matrix too much, let's convert it back and compare to the original.\n",
    "reconstructed_matrix = t3f.full(a_tt)\n",
    "print(reconstructed_matrix, a_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same idea applies to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random dense tensor of size 3 x 2 x 2.\n",
    "a_dense = np.random.randn(3, 2, 2)\n",
    "# Convert the tensor into the TT-format with TT-rank = 3.\n",
    "a_tt = t3f.to_tt_tensor(a_dense, max_tt_rank=3)\n",
    "# Print TT-cores that compactly represent the tensor.\n",
    "print(a_tt.tt_cores)\n",
    "# To check that the convertions into the TT-format didn't change the tensor too much, let's convert it back and compare to the original.\n",
    "reconstructed_tensor = t3f.full(a_tt)\n",
    "print(reconstructed_tensor, a_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T3F is a library of different operations that can be applied to the tensors in the TT-format (by working directly with the compact representation, i.e. without the need to materialize the tensors themself).\n",
    "Here are some basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random tensor of shape (3, 2, 2) directly in the TT-format (in contrast to generating a dense tensor and then converting it to TT).\n",
    "b_tt = t3f.random_tensor((3, 2, 2), tt_rank=3)\n",
    "# Compute the Frobenius norm of the tensor.\n",
    "norm = t3f.frobenius_norm(b_tt)\n",
    "print(norm)\n",
    "# Compute the TT-representation of the sum or elementwise product of two TT-tensors.\n",
    "sum = a_tt + b_tt\n",
    "prod = a_tt * b_tt\n",
    "twice_a = 2 * a_tt\n",
    "# Most operations on TT-tensors increase the TT-rank. After applying a seqeunce of operations the TT-rank can increase by too much and we may want to reduce it. To do that there is a rounding operation, which finds the closes tensor to the given one of a smaller rank.\n",
    "rounded_prod = t3f.round(prod)\n",
    "CHECK RANKS!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
